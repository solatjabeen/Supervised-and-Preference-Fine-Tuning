{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829e0f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:47:50.918060: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-14 09:47:50.924562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747198070.932713 1023751 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747198070.935143 1023751 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-14 09:47:50.943890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "import psutil\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef75cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\n",
    "torch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad49ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_footprint():\n",
    "    # GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n",
    "        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n",
    "    else:\n",
    "        print(\"[GPU] No GPU detected.\")\n",
    "\n",
    "    # CPU memory usage\n",
    "    memory = psutil.virtual_memory()\n",
    "    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n",
    "    total_memory_gb = memory.total / (1024 ** 3)\n",
    "    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b6efd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"First example of blended_skill_talk training set:\")\\nprint(dataset[\\'train\\'][0])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"First example of blended_skill_talk training set:\")\n",
    "print(dataset['train'][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27738da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and tokenizer\n",
    "# dataset = load_dataset(\"daily_dialog\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, so we use eos_token\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate dialog turns into a single string for language modeling\n",
    "    texts = [\" \".join(dialog) for dialog in examples[\"dialog\"]]\n",
    "    return tokenizer(texts, truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"dialog\", \"act\", \"emotion\"])\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3032cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialog': [['Say , Jim , how about going for a few beers after dinner ? ',\n",
       "   ' You know that is tempting but is really not good for our fitness . ',\n",
       "   ' What do you mean ? It will help us to relax . ',\n",
       "   \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n",
       "   \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n",
       "   ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n",
       "   \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n",
       "   ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n",
       "   \" Good.Let ' s go now . \",\n",
       "   ' All right . ']],\n",
       " 'act': [[3, 4, 2, 2, 2, 3, 4, 1, 3, 4]],\n",
       " 'emotion': [[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18cd876d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1,\n",
       "   7146,\n",
       "   591,\n",
       "   29915,\n",
       "   276,\n",
       "   373,\n",
       "   7613,\n",
       "   1738,\n",
       "   259,\n",
       "   4874,\n",
       "   869,\n",
       "   739,\n",
       "   471,\n",
       "   577,\n",
       "   11660,\n",
       "   7176,\n",
       "   869,\n",
       "   306,\n",
       "   29915,\n",
       "   29885,\n",
       "   28043,\n",
       "   29899,\n",
       "   449,\n",
       "   869,\n",
       "   2803,\n",
       "   29915,\n",
       "   29879,\n",
       "   1284,\n",
       "   1749,\n",
       "   22091,\n",
       "   869,\n",
       "   259,\n",
       "   526,\n",
       "   896,\n",
       "   3474,\n",
       "   22091,\n",
       "   470,\n",
       "   263,\n",
       "   275,\n",
       "   280,\n",
       "   22091,\n",
       "   1577,\n",
       "   259,\n",
       "   1235,\n",
       "   592,\n",
       "   1074,\n",
       "   2023,\n",
       "   4874,\n",
       "   1919,\n",
       "   697,\n",
       "   3474,\n",
       "   12949,\n",
       "   322,\n",
       "   697,\n",
       "   263,\n",
       "   275,\n",
       "   280,\n",
       "   12949,\n",
       "   869,\n",
       "   259,\n",
       "   3431,\n",
       "   869,\n",
       "   1205,\n",
       "   508,\n",
       "   306,\n",
       "   11302,\n",
       "   590,\n",
       "   12949,\n",
       "   411,\n",
       "   366,\n",
       "   1577,\n",
       "   306,\n",
       "   5821,\n",
       "   278,\n",
       "   697,\n",
       "   2978,\n",
       "   278,\n",
       "   3474,\n",
       "   869,\n",
       "   306,\n",
       "   29915,\n",
       "   29885,\n",
       "   263,\n",
       "   16403,\n",
       "   11340,\n",
       "   261,\n",
       "   869,\n",
       "   306,\n",
       "   2337,\n",
       "   679,\n",
       "   4799,\n",
       "   29879,\n",
       "   860,\n",
       "   322,\n",
       "   508,\n",
       "   2360,\n",
       "   26681,\n",
       "   2745,\n",
       "   1156,\n",
       "   306,\n",
       "   29915,\n",
       "   345,\n",
       "   2982,\n",
       "   287,\n",
       "   869,\n",
       "   259,\n",
       "   393,\n",
       "   29915,\n",
       "   29879,\n",
       "   2691,\n",
       "   869,\n",
       "   306,\n",
       "   29915,\n",
       "   29881,\n",
       "   763,\n",
       "   304,\n",
       "   367,\n",
       "   373,\n",
       "   278,\n",
       "   263,\n",
       "   275,\n",
       "   280,\n",
       "   8763,\n",
       "   869,\n",
       "   739,\n",
       "   29915,\n",
       "   29879,\n",
       "   6775,\n",
       "   304,\n",
       "   679,\n",
       "   297,\n",
       "   322,\n",
       "   714,\n",
       "   869,\n",
       "   259,\n",
       "   3969,\n",
       "   869,\n",
       "   6804,\n",
       "   4091,\n",
       "   591,\n",
       "   1925,\n",
       "   1749,\n",
       "   7245,\n",
       "   29887,\n",
       "   482,\n",
       "   1577,\n",
       "   259,\n",
       "   306,\n",
       "   1348,\n",
       "   278,\n",
       "   7968,\n",
       "   8677,\n",
       "   29899,\n",
       "   265,\n",
       "   19548,\n",
       "   508,\n",
       "   748,\n",
       "   297,\n",
       "   278,\n",
       "   18702,\n",
       "   29078,\n",
       "   358,\n",
       "   1919,\n",
       "   322,\n",
       "   278,\n",
       "   4045,\n",
       "   508,\n",
       "   748,\n",
       "   1090,\n",
       "   278,\n",
       "   12949,\n",
       "   869,\n",
       "   259,\n",
       "   1781,\n",
       "   2969,\n",
       "   869,\n",
       "   259,\n",
       "   1016,\n",
       "   29915,\n",
       "   29873,\n",
       "   9566,\n",
       "   304,\n",
       "   3013,\n",
       "   278,\n",
       "   12949,\n",
       "   1339,\n",
       "   29873,\n",
       "   373,\n",
       "   869,\n",
       "   259,\n",
       "   3431,\n",
       "   869,\n",
       "   7963,\n",
       "   372,\n",
       "   29915,\n",
       "   29879,\n",
       "   263,\n",
       "   21246,\n",
       "   17487,\n",
       "   869,\n",
       "   259,\n",
       "   4874,\n",
       "   1738,\n",
       "   259,\n",
       "   322,\n",
       "   694,\n",
       "   4147,\n",
       "   547,\n",
       "   414,\n",
       "   869,\n",
       "   259,\n",
       "   9360,\n",
       "   1919,\n",
       "   366,\n",
       "   505,\n",
       "   2086,\n",
       "   8775,\n",
       "   310,\n",
       "   385,\n",
       "   28038,\n",
       "   869,\n",
       "   29871]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fbb620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprocessed_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\\'dialog\\', \\'act\\', \\'emotion\\'])\\nsmall_train_dataset = processed_datasets[\"train\"].shuffle(seed=42).select(range(1000))\\nsmall_eval_dataset = processed_datasets[\"validation\"].shuffle(seed=42).select(range(100)) # Using validation for a small eval set\\n\\nprint_memory_footprint()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "processed_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['dialog', 'act', 'emotion'])\n",
    "small_train_dataset = processed_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = processed_datasets[\"validation\"].shuffle(seed=42).select(range(100)) # Using validation for a small eval set\n",
    "\n",
    "print_memory_footprint()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24655c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration for causal language modeling\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    ")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f418c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[GPU] Memory Allocated: 2.06 GB, Cached: 2.14 GB\n",
      "[CPU] Memory Usage: 4.58 GB / 62.56 GB\n"
     ]
    }
   ],
   "source": [
    "# Dynamic device assignment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained GPT-2 language model\n",
    "# Load pre-trained GPT-2 model\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                                             torch_dtype=torch.bfloat16,).to(device)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient checkpointing if you run into memory issues\n",
    "#model.gradient_checkpointing_enable()\n",
    "\n",
    "# %%\n",
    "# Print the model's architecture to inspect the names of the modules\n",
    "#print(model)\n",
    "\n",
    "# %%\n",
    "print_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0863d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU] Memory Allocated: 2.06 GB, Cached: 2.14 GB\n",
      "[CPU] Memory Usage: 4.58 GB / 62.56 GB\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Data collator for language modeling (masks tokens for prediction)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlm=False for causal LM\n",
    "\n",
    "# Perplexity as metric\n",
    "#perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    shift_logits = torch.tensor(logits[:, :-1, :])\n",
    "    shift_labels = torch.tensor(labels[:, 1:])\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    ppl = torch.exp(loss).item()\n",
    "    return {\"perplexity\": ppl}\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-dialog-finetuned\",\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=250,\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,  # Enable bfloat16\n",
    "    fp16=False,  # Disable fp16 to avoid conflicts\n",
    ")\n",
    "# %%\n",
    "print_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0355f85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1023751/3187942852.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "class MemoryCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        print(\"\\nMemory footprint after evaluation:\")\n",
    "        print_memory_footprint()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    #eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    "    callbacks=[MemoryCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5daa30a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GPT-2 Before Fine-tuning ---\n",
      "Input: Hello, how are you?\n",
      "Generated Response (Before): Hello, how are you?\n",
      "I am fine, thank you.\n",
      "How are you? I am fine, thank you.\n",
      "Can you repeat that again?\n",
      "Can you repeat that again, please?\n",
      "Can you repeat that again,\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- GPT-2 Before Fine-tuning ---\")\n",
    "model_before_finetune = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "             torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "input_text_before = \"Hello, how are you?\"\n",
    "input_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Input: {input_text_before}\")\n",
    "output_before = model_before_finetune.generate(\n",
    "    input_ids_before,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "generated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\n",
    "print(f\"Generated Response (Before): {generated_response_before}\")\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b21b2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TinyLlama Before Fine-tuning ---\n",
      "Input: The USA celebrate independence day on \n",
      "Generated Response (Before): The USA celebrate independence day on 4th July.\n",
      "\n",
      "2. The USA celebrate independence day on 4th July.\n",
      "\n",
      "3. The USA celebrate independence day on 4th July.\n",
      "\n",
      "4. The\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- TinyLlama Before Fine-tuning ---\")\n",
    "#model_before_finetune = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "input_text_before = \"The USA celebrate independence day on \"\n",
    "input_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Input: {input_text_before}\")\n",
    "output_before = model_before_finetune.generate(\n",
    "    input_ids_before,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "generated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\n",
    "print(f\"Generated Response (Before): {generated_response_before}\")\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb4c8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(p.requires_grad for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e84bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16670' max='16670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16670/16670 35:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>2.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>2.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>2.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>2.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>2.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>2.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>2.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>2.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>2.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>2.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>2.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>2.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>2.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>2.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>2.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>2.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>2.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>2.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>2.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>2.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>2.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>2.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>2.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.044400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16670, training_loss=2.068904502790848, metrics={'train_runtime': 2134.2274, 'train_samples_per_second': 46.855, 'train_steps_per_second': 7.811, 'total_flos': 1.6003618348577587e+17, 'train_loss': 2.068904502790848, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe5f6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('TinyLlama-dailydialog-1000-May25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bedb9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU] Memory Allocated: 2.08 GB, Cached: 13.16 GB\n",
      "[CPU] Memory Usage: 5.12 GB / 62.56 GB\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6acae58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response:\n",
      "Input: Hello, how are you?\n",
      "Generated Response: Hello, how are you?   I'm fine.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?  \n",
      "[GPU] Memory Allocated: 6.19 GB, Cached: 13.16 GB\n",
      "[CPU] Memory Usage: 5.90 GB / 62.56 GB\n"
     ]
    }
   ],
   "source": [
    "# For generation, we'll load the model and use the `generate` method\n",
    "model_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-dailydialog-1000-May25').to(device)\n",
    "#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n",
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nGenerating response:\")\n",
    "output = model_for_generation.generate(\n",
    "    input_ids,\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Response: {generated_response}\")\n",
    "\n",
    "print_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64411ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response:\n",
      "Input: The USA celebrate independence day on \n",
      "Generated Response: The USA celebrate independence day on 4th July .   What do you think about it ?   I think it's a good idea .   Why ?   Because it's a good day to celebrate .   I agree .   What do you think about the flag ?   I think it's a good flag .   What do you think about the national anthem ?   I think it's a good anthem .   What do you think about the national holiday ?   I think it's a good holiday .   What do you think about the people ?   I think they are very friendly .   What do you think about the government ?   I think they are very good .   What do\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The USA celebrate independence day on \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nGenerating response:\")\n",
    "output = model_for_generation.generate(\n",
    "    input_ids,\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Response: {generated_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
