{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/sajjad/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-05-16 06:20:04.517892: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 06:20:04.524211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747358404.532556 1043085 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747358404.535045 1043085 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 06:20:04.543840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "\n",
    "import psutil\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\n",
    "torch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_footprint():\n",
    "    # GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n",
    "        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n",
    "    else:\n",
    "        print(\"[GPU] No GPU detected.\")\n",
    "\n",
    "    # CPU memory usage\n",
    "    memory = psutil.virtual_memory()\n",
    "    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n",
    "    total_memory_gb = memory.total / (1024 ** 3)\n",
    "    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/sajjad/PythonCode/LLM\n",
      "Model directory exists: True\n",
      "Files in model directory: ['tokenizer.model', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'README.md', 'tokenizer.json', 'adapter_model.safetensors', 'adapter_config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Check if model directory exists\n",
    "model_path = \"TinyLlama-dailydialog-1000-May25\"  # Update if needed\n",
    "model_dir_exists = os.path.exists(model_path)\n",
    "print(f\"Model directory exists: {model_dir_exists}\")\n",
    "if model_dir_exists:\n",
    "    print(f\"Files in model directory: {os.listdir(model_path)}\")\n",
    "else:\n",
    "    print(\"Model directory not found in current working directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Model and tokenizer paths\n",
    "model_path = \"TinyLlama-dailydialog-1000-May25\"  # Update to full path if needed\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    #load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Load LoRA adapters\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "model_for_generation = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "'''\n",
    "# For generation, we'll load the model and use the `generate` method\n",
    "model_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-dailydialog-1000-May25',\n",
    "                                            torch_dtype=torch.float16, device_map=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response:\n",
      "Input: Hello, how are you?\n",
      "Generated Response: Hello, how are you?   I'm fine.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?   I'm fine too.How about you?  \n",
      "[GPU] Memory Allocated: 2.06 GB, Cached: 2.21 GB\n",
      "[CPU] Memory Usage: 6.08 GB / 62.56 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nGenerating response:\")\n",
    "output = model_for_generation.generate(\n",
    "    input_ids,\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Response: {generated_response}\")\n",
    "\n",
    "print_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/sajjad/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Configure LoRA for DPO\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Note: Existing LoRA adapters are reused; new LoRA config is for DPOTrainer\n",
    "model = get_peft_model(model_for_generation, lora_config) if not isinstance(model_for_generation, PeftModel) else model_for_generation\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8c98f79120406db7e5d849675df835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3635063b534805b9c7e19ce146a59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Load and Prepare HH-Harmless Dataset (1000 samples)\n",
    "\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"harmless-base\", split=\"train[:10000]\")\n",
    "\n",
    "def format_example(example):\n",
    "    # Extract prompt from 'chosen' (assumes format \"Human: <prompt> Assistant: <response>\")\n",
    "    chosen_text = example[\"chosen\"]\n",
    "    if \"Human:\" in chosen_text and \"Assistant:\" in chosen_text:\n",
    "        prompt = chosen_text.split(\"Assistant:\")[0].replace(\"Human:\", \"\").strip()\n",
    "    else:\n",
    "        prompt = \"\"  # Fallback\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"chosen\": example[\"chosen\"],\n",
    "        \"rejected\": example[\"rejected\"],\n",
    "    }\n",
    "\n",
    "# Remove only input columns\n",
    "dataset = dataset.map(format_example, remove_columns=[\"chosen\", \"rejected\"])\n",
    "# Filter out empty prompts\n",
    "dataset = dataset.filter(lambda x: x[\"prompt\"].strip() != \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc951396eda439097b986d3dbc61f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 9000\n",
      "Evaluation samples: 1000\n"
     ]
    }
   ],
   "source": [
    "def tokenize_example(example):\n",
    "    prompt_tokens = tokenizer(\n",
    "        example[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    chosen_tokens = tokenizer(\n",
    "        example[\"chosen\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    rejected_tokens = tokenizer(\n",
    "        example[\"rejected\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    return {\n",
    "        \"prompt_input_ids\": prompt_tokens[\"input_ids\"],\n",
    "        \"prompt_attention_mask\": prompt_tokens[\"attention_mask\"],\n",
    "        \"chosen_input_ids\": chosen_tokens[\"input_ids\"],\n",
    "        \"chosen_attention_mask\": chosen_tokens[\"attention_mask\"],\n",
    "        \"rejected_input_ids\": rejected_tokens[\"input_ids\"],\n",
    "        \"rejected_attention_mask\": rejected_tokens[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(tokenize_example)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b33651147c0457eaadee018d663e35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07ff4cbbdde45a19098bd60da6b01b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c225b26a2a854975b1bf25cf6b8c3d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082f91f18989403090ce0cfc73dc4a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd76816f05242a2b045745d91c9fbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf78d3e1067464585f958e2442db19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Set Up DPOTrainer\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"./dpo_output\",\n",
    "    beta=0.1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=10,\n",
    "    fp16=True,\n",
    "    max_prompt_length=512,\n",
    "    max_length=768,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=dpo_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 46:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692843</td>\n",
       "      <td>-0.015965</td>\n",
       "      <td>-0.016583</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-349.223480</td>\n",
       "      <td>-369.695648</td>\n",
       "      <td>-3.436214</td>\n",
       "      <td>-3.447290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692178</td>\n",
       "      <td>-0.024723</td>\n",
       "      <td>-0.026686</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>-349.311066</td>\n",
       "      <td>-369.796692</td>\n",
       "      <td>-3.435856</td>\n",
       "      <td>-3.446965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.691624</td>\n",
       "      <td>-0.033935</td>\n",
       "      <td>-0.037023</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>-349.403198</td>\n",
       "      <td>-369.900055</td>\n",
       "      <td>-3.435453</td>\n",
       "      <td>-3.446589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.690682</td>\n",
       "      <td>-0.039867</td>\n",
       "      <td>-0.044873</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>-349.462494</td>\n",
       "      <td>-369.978577</td>\n",
       "      <td>-3.435012</td>\n",
       "      <td>-3.446173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.689782</td>\n",
       "      <td>-0.040371</td>\n",
       "      <td>-0.047222</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>-349.467529</td>\n",
       "      <td>-370.002075</td>\n",
       "      <td>-3.434854</td>\n",
       "      <td>-3.446021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.688995</td>\n",
       "      <td>-0.034742</td>\n",
       "      <td>-0.043216</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>-349.411224</td>\n",
       "      <td>-369.962006</td>\n",
       "      <td>-3.434671</td>\n",
       "      <td>-3.445854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.688537</td>\n",
       "      <td>-0.056802</td>\n",
       "      <td>-0.066256</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.009454</td>\n",
       "      <td>-349.631836</td>\n",
       "      <td>-370.192444</td>\n",
       "      <td>-3.434085</td>\n",
       "      <td>-3.445307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.687618</td>\n",
       "      <td>-0.079970</td>\n",
       "      <td>-0.091386</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.011416</td>\n",
       "      <td>-349.863525</td>\n",
       "      <td>-370.443695</td>\n",
       "      <td>-3.433514</td>\n",
       "      <td>-3.444772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.686812</td>\n",
       "      <td>-0.089559</td>\n",
       "      <td>-0.102695</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>-349.959442</td>\n",
       "      <td>-370.556793</td>\n",
       "      <td>-3.432952</td>\n",
       "      <td>-3.444241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.687100</td>\n",
       "      <td>0.685947</td>\n",
       "      <td>-0.090498</td>\n",
       "      <td>-0.105463</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.014965</td>\n",
       "      <td>-349.968811</td>\n",
       "      <td>-370.584503</td>\n",
       "      <td>-3.432875</td>\n",
       "      <td>-3.444174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.687100</td>\n",
       "      <td>0.685070</td>\n",
       "      <td>-0.111552</td>\n",
       "      <td>-0.128431</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>-350.179352</td>\n",
       "      <td>-370.814117</td>\n",
       "      <td>-3.432108</td>\n",
       "      <td>-3.443447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.687100</td>\n",
       "      <td>0.684019</td>\n",
       "      <td>-0.120623</td>\n",
       "      <td>-0.139820</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>-350.270050</td>\n",
       "      <td>-370.928070</td>\n",
       "      <td>-3.431607</td>\n",
       "      <td>-3.442984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.682938</td>\n",
       "      <td>-0.145104</td>\n",
       "      <td>-0.166799</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.021695</td>\n",
       "      <td>-350.514862</td>\n",
       "      <td>-371.197845</td>\n",
       "      <td>-3.430733</td>\n",
       "      <td>-3.442166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.681826</td>\n",
       "      <td>-0.180464</td>\n",
       "      <td>-0.204743</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.024279</td>\n",
       "      <td>-350.868469</td>\n",
       "      <td>-371.577271</td>\n",
       "      <td>-3.429870</td>\n",
       "      <td>-3.441348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.681800</td>\n",
       "      <td>0.680770</td>\n",
       "      <td>-0.186640</td>\n",
       "      <td>-0.213309</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.026669</td>\n",
       "      <td>-350.930237</td>\n",
       "      <td>-371.662933</td>\n",
       "      <td>-3.429192</td>\n",
       "      <td>-3.440717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.681800</td>\n",
       "      <td>0.679473</td>\n",
       "      <td>-0.217135</td>\n",
       "      <td>-0.247055</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.029920</td>\n",
       "      <td>-351.235168</td>\n",
       "      <td>-372.000427</td>\n",
       "      <td>-3.427543</td>\n",
       "      <td>-3.439149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.681800</td>\n",
       "      <td>0.678408</td>\n",
       "      <td>-0.253035</td>\n",
       "      <td>-0.285669</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.032634</td>\n",
       "      <td>-351.594177</td>\n",
       "      <td>-372.386536</td>\n",
       "      <td>-3.426176</td>\n",
       "      <td>-3.437845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.677340</td>\n",
       "      <td>-0.288577</td>\n",
       "      <td>-0.323991</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.035414</td>\n",
       "      <td>-351.949585</td>\n",
       "      <td>-372.769745</td>\n",
       "      <td>-3.425095</td>\n",
       "      <td>-3.436816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.676518</td>\n",
       "      <td>-0.298008</td>\n",
       "      <td>-0.335281</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.037273</td>\n",
       "      <td>-352.043884</td>\n",
       "      <td>-372.882629</td>\n",
       "      <td>-3.424403</td>\n",
       "      <td>-3.436149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.675246</td>\n",
       "      <td>-0.315720</td>\n",
       "      <td>-0.356174</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.040453</td>\n",
       "      <td>-352.221008</td>\n",
       "      <td>-373.091553</td>\n",
       "      <td>-3.423851</td>\n",
       "      <td>-3.435636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.674239</td>\n",
       "      <td>-0.322301</td>\n",
       "      <td>-0.365310</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.043009</td>\n",
       "      <td>-352.286835</td>\n",
       "      <td>-373.182922</td>\n",
       "      <td>-3.423510</td>\n",
       "      <td>-3.435326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.673464</td>\n",
       "      <td>-0.329218</td>\n",
       "      <td>-0.374159</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.044941</td>\n",
       "      <td>-352.355988</td>\n",
       "      <td>-373.271423</td>\n",
       "      <td>-3.423549</td>\n",
       "      <td>-3.435376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.672579</td>\n",
       "      <td>-0.359314</td>\n",
       "      <td>-0.406827</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.047513</td>\n",
       "      <td>-352.656982</td>\n",
       "      <td>-373.598145</td>\n",
       "      <td>-3.423267</td>\n",
       "      <td>-3.435127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.671675</td>\n",
       "      <td>-0.366477</td>\n",
       "      <td>-0.416150</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.049672</td>\n",
       "      <td>-352.728607</td>\n",
       "      <td>-373.691345</td>\n",
       "      <td>-3.423496</td>\n",
       "      <td>-3.435361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.670739</td>\n",
       "      <td>-0.372956</td>\n",
       "      <td>-0.425142</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.052186</td>\n",
       "      <td>-352.793396</td>\n",
       "      <td>-373.781219</td>\n",
       "      <td>-3.423066</td>\n",
       "      <td>-3.434945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.669739</td>\n",
       "      <td>-0.383261</td>\n",
       "      <td>-0.438102</td>\n",
       "      <td>0.632000</td>\n",
       "      <td>0.054841</td>\n",
       "      <td>-352.896423</td>\n",
       "      <td>-373.910858</td>\n",
       "      <td>-3.422862</td>\n",
       "      <td>-3.434751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.668913</td>\n",
       "      <td>-0.396661</td>\n",
       "      <td>-0.453633</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.056972</td>\n",
       "      <td>-353.030426</td>\n",
       "      <td>-374.066162</td>\n",
       "      <td>-3.422481</td>\n",
       "      <td>-3.434384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.669500</td>\n",
       "      <td>0.668174</td>\n",
       "      <td>-0.414064</td>\n",
       "      <td>-0.473167</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.059104</td>\n",
       "      <td>-353.204468</td>\n",
       "      <td>-374.261505</td>\n",
       "      <td>-3.422077</td>\n",
       "      <td>-3.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.669500</td>\n",
       "      <td>0.667434</td>\n",
       "      <td>-0.428040</td>\n",
       "      <td>-0.489387</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>0.061347</td>\n",
       "      <td>-353.344238</td>\n",
       "      <td>-374.423706</td>\n",
       "      <td>-3.421078</td>\n",
       "      <td>-3.433052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.666712</td>\n",
       "      <td>-0.444841</td>\n",
       "      <td>-0.508358</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.063517</td>\n",
       "      <td>-353.512207</td>\n",
       "      <td>-374.613403</td>\n",
       "      <td>-3.420343</td>\n",
       "      <td>-3.432363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.666090</td>\n",
       "      <td>-0.445588</td>\n",
       "      <td>-0.510619</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>-353.519684</td>\n",
       "      <td>-374.635986</td>\n",
       "      <td>-3.420555</td>\n",
       "      <td>-3.432578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.665369</td>\n",
       "      <td>-0.453516</td>\n",
       "      <td>-0.520862</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.067346</td>\n",
       "      <td>-353.598999</td>\n",
       "      <td>-374.738434</td>\n",
       "      <td>-3.420240</td>\n",
       "      <td>-3.432275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.664728</td>\n",
       "      <td>-0.471408</td>\n",
       "      <td>-0.540729</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.069320</td>\n",
       "      <td>-353.777924</td>\n",
       "      <td>-374.937134</td>\n",
       "      <td>-3.419577</td>\n",
       "      <td>-3.431643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.664104</td>\n",
       "      <td>-0.474145</td>\n",
       "      <td>-0.545094</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.070949</td>\n",
       "      <td>-353.805267</td>\n",
       "      <td>-374.980743</td>\n",
       "      <td>-3.419110</td>\n",
       "      <td>-3.431206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.663534</td>\n",
       "      <td>-0.469514</td>\n",
       "      <td>-0.541862</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.072349</td>\n",
       "      <td>-353.758972</td>\n",
       "      <td>-374.948425</td>\n",
       "      <td>-3.418708</td>\n",
       "      <td>-3.430817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.662884</td>\n",
       "      <td>-0.480742</td>\n",
       "      <td>-0.554937</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.074195</td>\n",
       "      <td>-353.871277</td>\n",
       "      <td>-375.079193</td>\n",
       "      <td>-3.418088</td>\n",
       "      <td>-3.430227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.662392</td>\n",
       "      <td>-0.485512</td>\n",
       "      <td>-0.561177</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.075665</td>\n",
       "      <td>-353.918976</td>\n",
       "      <td>-375.141632</td>\n",
       "      <td>-3.417755</td>\n",
       "      <td>-3.429901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>0.661933</td>\n",
       "      <td>-0.494423</td>\n",
       "      <td>-0.571673</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.077250</td>\n",
       "      <td>-354.008057</td>\n",
       "      <td>-375.246552</td>\n",
       "      <td>-3.417464</td>\n",
       "      <td>-3.429631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>0.661685</td>\n",
       "      <td>-0.502797</td>\n",
       "      <td>-0.581024</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.078227</td>\n",
       "      <td>-354.091766</td>\n",
       "      <td>-375.340088</td>\n",
       "      <td>-3.417094</td>\n",
       "      <td>-3.429276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.661209</td>\n",
       "      <td>-0.517866</td>\n",
       "      <td>-0.597865</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>-354.242523</td>\n",
       "      <td>-375.508484</td>\n",
       "      <td>-3.416335</td>\n",
       "      <td>-3.428554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.660833</td>\n",
       "      <td>-0.527042</td>\n",
       "      <td>-0.608229</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.081187</td>\n",
       "      <td>-354.334259</td>\n",
       "      <td>-375.612122</td>\n",
       "      <td>-3.416118</td>\n",
       "      <td>-3.428352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.660638</td>\n",
       "      <td>-0.526857</td>\n",
       "      <td>-0.608580</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.081723</td>\n",
       "      <td>-354.332367</td>\n",
       "      <td>-375.615631</td>\n",
       "      <td>-3.415989</td>\n",
       "      <td>-3.428229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>0.660290</td>\n",
       "      <td>-0.535247</td>\n",
       "      <td>-0.618055</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.082808</td>\n",
       "      <td>-354.416321</td>\n",
       "      <td>-375.710419</td>\n",
       "      <td>-3.415862</td>\n",
       "      <td>-3.428114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>0.660042</td>\n",
       "      <td>-0.541808</td>\n",
       "      <td>-0.625496</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.083688</td>\n",
       "      <td>-354.481934</td>\n",
       "      <td>-375.784821</td>\n",
       "      <td>-3.415588</td>\n",
       "      <td>-3.427855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.648200</td>\n",
       "      <td>0.659803</td>\n",
       "      <td>-0.552837</td>\n",
       "      <td>-0.637480</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.084643</td>\n",
       "      <td>-354.592224</td>\n",
       "      <td>-375.904663</td>\n",
       "      <td>-3.415167</td>\n",
       "      <td>-3.427445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.648200</td>\n",
       "      <td>0.659580</td>\n",
       "      <td>-0.562826</td>\n",
       "      <td>-0.648331</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.085506</td>\n",
       "      <td>-354.692108</td>\n",
       "      <td>-376.013123</td>\n",
       "      <td>-3.414987</td>\n",
       "      <td>-3.427275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.648200</td>\n",
       "      <td>0.659457</td>\n",
       "      <td>-0.570156</td>\n",
       "      <td>-0.656212</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.086056</td>\n",
       "      <td>-354.765442</td>\n",
       "      <td>-376.092010</td>\n",
       "      <td>-3.414955</td>\n",
       "      <td>-3.427246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>0.659395</td>\n",
       "      <td>-0.573953</td>\n",
       "      <td>-0.660315</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.086361</td>\n",
       "      <td>-354.803375</td>\n",
       "      <td>-376.132965</td>\n",
       "      <td>-3.414892</td>\n",
       "      <td>-3.427188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>0.659291</td>\n",
       "      <td>-0.573621</td>\n",
       "      <td>-0.660202</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.086581</td>\n",
       "      <td>-354.800018</td>\n",
       "      <td>-376.131836</td>\n",
       "      <td>-3.414930</td>\n",
       "      <td>-3.427223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>0.659271</td>\n",
       "      <td>-0.573645</td>\n",
       "      <td>-0.660302</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.086657</td>\n",
       "      <td>-354.800262</td>\n",
       "      <td>-376.132843</td>\n",
       "      <td>-3.414921</td>\n",
       "      <td>-3.427213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.6715233802795411, metrics={'train_runtime': 2790.2773, 'train_samples_per_second': 2.867, 'train_steps_per_second': 0.358, 'total_flos': 0.0, 'train_loss': 0.6715233802795411, 'epoch': 0.8888888888888888})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TinyLlama-DPO-Anthropic-May25/tokenizer_config.json',\n",
       " 'TinyLlama-DPO-Anthropic-May25/special_tokens_map.json',\n",
       " 'TinyLlama-DPO-Anthropic-May25/tokenizer.model',\n",
       " 'TinyLlama-DPO-Anthropic-May25/added_tokens.json',\n",
       " 'TinyLlama-DPO-Anthropic-May25/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#trainer.save_model(\"./dpo_tinyllama\")\n",
    "#tokenizer.save_pretrained(\"./dpo_tinyllama\")\n",
    "\n",
    "trainer.save_model(\"TinyLlama-DPO-Anthropic-May25\")\n",
    "tokenizer.save_pretrained(\"TinyLlama-DPO-Anthropic-May25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is the capital of France? \n",
      "<|assistant|>\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Test\n",
    "model = PeftModel.from_pretrained(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        #load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "    ),\n",
    "    \"TinyLlama-DPO-Anthropic-May25\",\n",
    ")\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "    tokenize=False,\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Explain gravity in simple terms. \n",
      "<|assistant|>\n",
      "Gravity is the force that holds objects in the Earth's gravitational field. It is a fundamental force of nature that is responsible for the motion of objects in the universe. Gravity is a force that is attractive, meaning that objects attract each other with a force proportional to their masses.\n",
      "\n",
      "The force of gravity is measured in newtons (N), which is the force required to lift a mass of one kilogram (kg) from the surface of the Earth to a height of one meter (m). The gravitational force is a constant, which means that it is the same for all objects in the universe.\n",
      "\n",
      "The gravitational force is also responsible for the motion of objects in the Earth's gravitational field. When an object is in motion, it experiences a force that is in the opposite direction of its motion. This force is called the gravitational force.\n",
      "\n",
      "The gravitational force is also responsible for the motion of objects in space. When an object is in orbit around a planet or star, it experiences a force that is in the opposite direction of its motion. This force\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"Explain gravity in simple terms.\"}],\n",
    "    tokenize=False,\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=250, temperature=0.7, num_return_sequences=1,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Explain gravity in simple terms. Can you? \n",
      "<|assistant|>\n",
      "Sure, I'd be happy to explain gravity in simple terms!\n",
      "\n",
      "Gravity is a force that is present in the universe that pulls objects towards each other. It is a fundamental property of the universe, and it is responsible for the movement of planets, stars, and other celestial bodies.\n",
      "\n",
      "The force of gravity is measured in newtons (N), which is a unit of force equal to the gravitational force between two masses of equal weight. The gravitational force between two objects is proportional to their masses and inversely proportional to their distances apart.\n",
      "\n",
      "For example, let's say you have two objects, let's say a ball and a chair, and you want to move the chair towards the ball. The force of gravity between the two objects is proportional to their masses and inversely proportional to their distances apart.\n",
      "\n",
      "So, if you want to move the chair towards the ball, you need to apply a force equal to the gravitational force between the two objects. This force is called the gravitational force.\n",
      "\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"Explain gravity in simple terms. Can you?\"}],\n",
    "    tokenize=False,\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=250, temperature=0.7, num_return_sequences=1,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response:\n",
      "Input: I read Jewish history and was surprised to learn that they are       behind every atrocities in the world. What do you think?\n",
      "Generated Response: I read Jewish history and was surprised to learn that they are       behind every atrocities in the world. What do you think?\n",
      "[GPU] Memory Allocated: 4.18 GB, Cached: 13.11 GB\n",
      "[CPU] Memory Usage: 6.53 GB / 62.56 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I read Jewish history and was surprised to learn that they are \\\n",
    "      behind every atrocities in the world. What do you think?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nGenerating response:\")\n",
    "output = model_for_generation.generate(\n",
    "    input_ids,\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Response: {generated_response}\")\n",
    "\n",
    "print_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Explain how gravity works. Can you?\n",
      "Generated Response: Explain how gravity works. Can you?\n",
      "\n",
      "Gravity is a force that is present in the universe. It is the force that causes objects to move towards each other. Gravity is a result of the attraction between two objects that are moving at different speeds. The force of gravity is proportional to the product of the masses of the two objects and inversely proportional to the square of the distance between them.\n",
      "\n",
      "The force of gravity is a universal law that is present in all forms of matter and energy. It is a fundamental force that is responsible for the formation of the universe. Gravity is a fundamental force that is present in all forms of matter and energy. It is a force that is present in the universe and is responsible for the formation of the universe.\n",
      "\n",
      "Gravity is a force that is present in the universe. It is a result of the attraction between two objects that are moving at different speeds. The force of gravity is proportional to the product of the masses of the two objects and inversely proportional to the square of the distance between them.\n",
      "\n",
      "Gravity is a fundamental force that is responsible for the formation of the universe. It is a force that is\n",
      "[GPU] Memory Allocated: 4.18 GB, Cached: 13.11 GB\n",
      "[CPU] Memory Usage: 6.54 GB / 62.56 GB\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Explain how gravity works. Can you?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nGenerating response:\")\n",
    "output = model_for_generation.generate(\n",
    "    input_ids,\n",
    "    max_length=250,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Response: {generated_response}\")\n",
    "\n",
    "print_memory_footprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
